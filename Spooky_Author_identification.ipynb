{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一共有三个作者，应该是用rnn效果最好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'train.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...\n",
       "1  id24541  If a fire wanted fanning, it could readily be ...\n",
       "2  id00134  And when they had broken down the frail door t...\n",
       "3  id27757  While I was thinking how I should possibly man...\n",
       "4  id04081  I am not sure to what limit his knowledge may ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'test.csv'\n",
    "test_data = pd.read_csv(data_path)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除停止词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abst',\n",
       " 'across',\n",
       " 'act',\n",
       " 'actually',\n",
       " 'added',\n",
       " 'adj']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = [ line.rstrip() for line in open('stop_words.txt') ]\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_stop_words(words,stopwords=stopwords):\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            cleaned_words.append(word)\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规整数据，做一个字典，将所有出现的文字都encoding到字典中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counts = Counter()\n",
    "for line in data['text']:\n",
    "    words = line.lower().split()\n",
    "    #words = clean_stop_words(words)\n",
    "    counts.update(words)\n",
    "    \n",
    "for line in test_data['text']:\n",
    "    words = line.lower().split()\n",
    "    #words = clean_stop_words(words)\n",
    "    counts.update(words)\n",
    "\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 0)}\n",
    "\n",
    "def text_to_ints(text):#这里传进来的是一个text\n",
    "    text_ints = [vocab_to_int[word] for word in text.lower().split()]\n",
    "    text_ints = np.array(text_ints)\n",
    "    return text_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length: 53248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " 'of': 1,\n",
       " 'and': 2,\n",
       " 'to': 3,\n",
       " 'a': 4,\n",
       " 'i': 5,\n",
       " 'in': 6,\n",
       " 'was': 7,\n",
       " 'that': 8,\n",
       " 'my': 9,\n",
       " 'had': 10,\n",
       " 'with': 11,\n",
       " 'it': 12,\n",
       " 'he': 13,\n",
       " 'his': 14,\n",
       " 'as': 15,\n",
       " 'for': 16,\n",
       " 'which': 17,\n",
       " 'but': 18,\n",
       " 'at': 19,\n",
       " 'not': 20,\n",
       " 'from': 21,\n",
       " 'by': 22,\n",
       " 'is': 23,\n",
       " 'on': 24,\n",
       " 'this': 25,\n",
       " 'be': 26,\n",
       " 'were': 27,\n",
       " 'have': 28,\n",
       " 'me': 29,\n",
       " 'her': 30,\n",
       " 'all': 31,\n",
       " 'we': 32,\n",
       " 'or': 33,\n",
       " 'an': 34,\n",
       " 'no': 35,\n",
       " 'you': 36,\n",
       " 'one': 37,\n",
       " 'so': 38,\n",
       " 'they': 39,\n",
       " 'when': 40,\n",
       " 'been': 41,\n",
       " 'upon': 42,\n",
       " 'its': 43,\n",
       " 'could': 44,\n",
       " 'she': 45,\n",
       " 'would': 46,\n",
       " 'there': 47,\n",
       " 'their': 48,\n",
       " 'more': 49,\n",
       " 'some': 50,\n",
       " 'him': 51,\n",
       " 'what': 52,\n",
       " 'our': 53,\n",
       " 'now': 54,\n",
       " 'into': 55,\n",
       " 'are': 56,\n",
       " 'than': 57,\n",
       " 'very': 58,\n",
       " 'if': 59,\n",
       " 'who': 60,\n",
       " 'will': 61,\n",
       " 'only': 62,\n",
       " 'about': 63,\n",
       " 'these': 64,\n",
       " 'any': 65,\n",
       " 'up': 66,\n",
       " 'even': 67,\n",
       " 'did': 68,\n",
       " 'then': 69,\n",
       " 'yet': 70,\n",
       " 'your': 71,\n",
       " 'out': 72,\n",
       " 'before': 73,\n",
       " 'them': 74,\n",
       " 'after': 75,\n",
       " 'might': 76,\n",
       " 'old': 77,\n",
       " 'most': 78,\n",
       " 'like': 79,\n",
       " 'must': 80,\n",
       " 'through': 81,\n",
       " 'such': 82,\n",
       " 'should': 83,\n",
       " 'over': 84,\n",
       " 'first': 85,\n",
       " 'never': 86,\n",
       " 'made': 87,\n",
       " 'those': 88,\n",
       " 'seemed': 89,\n",
       " 'every': 90,\n",
       " 'man': 91,\n",
       " 'time': 92,\n",
       " 'while': 93,\n",
       " 'has': 94,\n",
       " 'great': 95,\n",
       " 'where': 96,\n",
       " 'found': 97,\n",
       " 'little': 98,\n",
       " 'other': 99,\n",
       " 'many': 100,\n",
       " 'saw': 101,\n",
       " 'do': 102,\n",
       " 'still': 103,\n",
       " 'me,': 104,\n",
       " 'said': 105,\n",
       " 'long': 106,\n",
       " 'much': 107,\n",
       " 'can': 108,\n",
       " 'own': 109,\n",
       " 'may': 110,\n",
       " 'how': 111,\n",
       " 'whose': 112,\n",
       " 'two': 113,\n",
       " 'us': 114,\n",
       " 'came': 115,\n",
       " 'down': 116,\n",
       " 'ever': 117,\n",
       " 'eyes': 118,\n",
       " 'see': 119,\n",
       " 'without': 120,\n",
       " 'am': 121,\n",
       " 'well': 122,\n",
       " 'thought': 123,\n",
       " 'being': 124,\n",
       " 'shall': 125,\n",
       " 'and,': 126,\n",
       " 'myself': 127,\n",
       " 'day': 128,\n",
       " 'life': 129,\n",
       " 'far': 130,\n",
       " 'once': 131,\n",
       " 'again': 132,\n",
       " 'felt': 133,\n",
       " 'night': 134,\n",
       " 'heard': 135,\n",
       " 'thus': 136,\n",
       " '.': 137,\n",
       " 'however,': 138,\n",
       " 'here': 139,\n",
       " 'me.': 140,\n",
       " 'few': 141,\n",
       " 'became': 142,\n",
       " 'among': 143,\n",
       " 'last': 144,\n",
       " 'though': 145,\n",
       " 'under': 146,\n",
       " 'thing': 147,\n",
       " 'whole': 148,\n",
       " 'it,': 149,\n",
       " 'know': 150,\n",
       " 'too': 151,\n",
       " 'left': 152,\n",
       " 'having': 153,\n",
       " 'within': 154,\n",
       " 'same': 155,\n",
       " 'half': 156,\n",
       " 'it.': 157,\n",
       " 'human': 158,\n",
       " 'nor': 159,\n",
       " 'new': 160,\n",
       " 'say': 161,\n",
       " 'let': 162,\n",
       " 'things': 163,\n",
       " 'each': 164,\n",
       " 'during': 165,\n",
       " 'make': 166,\n",
       " 'knew': 167,\n",
       " 'years': 168,\n",
       " 'nothing': 169,\n",
       " 'strange': 170,\n",
       " 'just': 171,\n",
       " 'men': 172,\n",
       " 'less': 173,\n",
       " 'three': 174,\n",
       " 'soon': 175,\n",
       " 'mind': 176,\n",
       " 'although': 177,\n",
       " 'back': 178,\n",
       " 'come': 179,\n",
       " 'him,': 180,\n",
       " 'since': 181,\n",
       " 'place': 182,\n",
       " 'almost': 183,\n",
       " 'heart': 184,\n",
       " 'gave': 185,\n",
       " 'seen': 186,\n",
       " 'light': 187,\n",
       " 'death': 188,\n",
       " 'himself': 189,\n",
       " 'took': 190,\n",
       " 'beyond': 191,\n",
       " 'good': 192,\n",
       " 'certain': 193,\n",
       " 'house': 194,\n",
       " 'love': 195,\n",
       " 'part': 196,\n",
       " 'looked': 197,\n",
       " 'way': 198,\n",
       " '\"i': 199,\n",
       " 'whom': 200,\n",
       " 'words': 201,\n",
       " 'take': 202,\n",
       " 'small': 203,\n",
       " 'around': 204,\n",
       " 'full': 205,\n",
       " 'something': 206,\n",
       " 'told': 207,\n",
       " 'find': 208,\n",
       " 'that,': 209,\n",
       " 'near': 210,\n",
       " 'head': 211,\n",
       " 'went': 212,\n",
       " 'door': 213,\n",
       " 'mr.': 214,\n",
       " 'moment': 215,\n",
       " 'above': 216,\n",
       " 'lay': 217,\n",
       " 'between': 218,\n",
       " 'another': 219,\n",
       " 'nature': 220,\n",
       " 'length': 221,\n",
       " \"an'\": 222,\n",
       " 'until': 223,\n",
       " 'away': 224,\n",
       " 'appeared': 225,\n",
       " 'young': 226,\n",
       " 'passed': 227,\n",
       " 'world': 228,\n",
       " 'high': 229,\n",
       " 'off': 230,\n",
       " 'indeed': 231,\n",
       " 'said,': 232,\n",
       " 'voice': 233,\n",
       " 'always': 234,\n",
       " 'cannot': 235,\n",
       " 'brought': 236,\n",
       " 'room': 237,\n",
       " 'think': 238,\n",
       " 'because': 239,\n",
       " 'then,': 240,\n",
       " 'turned': 241,\n",
       " 'against': 242,\n",
       " 'look': 243,\n",
       " 'often': 244,\n",
       " 'earth': 245,\n",
       " 'began': 246,\n",
       " 'air': 247,\n",
       " 'black': 248,\n",
       " 'fear': 249,\n",
       " 'also': 250,\n",
       " 'body': 251,\n",
       " 'dark': 252,\n",
       " 'idea': 253,\n",
       " 'rather': 254,\n",
       " 'sometimes': 255,\n",
       " 'spirit': 256,\n",
       " 'nearly': 257,\n",
       " 'both': 258,\n",
       " 'perhaps': 259,\n",
       " 'become': 260,\n",
       " 'known': 261,\n",
       " 'him.': 262,\n",
       " 'hand': 263,\n",
       " 'deep': 264,\n",
       " ',': 265,\n",
       " 'days': 266,\n",
       " 'called': 267,\n",
       " 'means': 268,\n",
       " 'tell': 269,\n",
       " 'open': 270,\n",
       " 'towards': 271,\n",
       " 'ancient': 272,\n",
       " 'end': 273,\n",
       " 'wild': 274,\n",
       " 'city': 275,\n",
       " 'horror': 276,\n",
       " 'feel': 277,\n",
       " 'soul': 278,\n",
       " 'go': 279,\n",
       " 'put': 280,\n",
       " 'raymond': 281,\n",
       " 'father': 282,\n",
       " 'which,': 283,\n",
       " 'least': 284,\n",
       " 'present': 285,\n",
       " 'form': 286,\n",
       " 'several': 287,\n",
       " 'kind': 288,\n",
       " 'was,': 289,\n",
       " 'name': 290,\n",
       " 'alone': 291,\n",
       " 'point': 292,\n",
       " 'sea': 293,\n",
       " 'face': 294,\n",
       " 'right': 295,\n",
       " 'now,': 296,\n",
       " 'already': 297,\n",
       " 'de': 298,\n",
       " 'stood': 299,\n",
       " 'why': 300,\n",
       " 'toward': 301,\n",
       " 'set': 302,\n",
       " 'grew': 303,\n",
       " 'taken': 304,\n",
       " 'hope': 305,\n",
       " 'but,': 306,\n",
       " 'fell': 307,\n",
       " 'them,': 308,\n",
       " 'spoke': 309,\n",
       " 'time,': 310,\n",
       " 'give': 311,\n",
       " 'large': 312,\n",
       " 'suddenly': 313,\n",
       " 'matter': 314,\n",
       " 'people': 315,\n",
       " 'quite': 316,\n",
       " 'terrible': 317,\n",
       " '\"the': 318,\n",
       " 'better': 319,\n",
       " 'lost': 320,\n",
       " 'thousand': 321,\n",
       " 'friend': 322,\n",
       " 'sound': 323,\n",
       " 'indeed,': 324,\n",
       " 'entered': 325,\n",
       " 'object': 326,\n",
       " 'sun': 327,\n",
       " 'beneath': 328,\n",
       " 'either': 329,\n",
       " 'general': 330,\n",
       " 'longer': 331,\n",
       " 'none': 332,\n",
       " 'itself': 333,\n",
       " 'you,': 334,\n",
       " 'believe': 335,\n",
       " 'say,': 336,\n",
       " 'manner': 337,\n",
       " 'dear': 338,\n",
       " 'person': 339,\n",
       " 'poor': 340,\n",
       " 'white': 341,\n",
       " 'water': 342,\n",
       " 'state': 343,\n",
       " 'dead': 344,\n",
       " 'return': 345,\n",
       " 'power': 346,\n",
       " 'till': 347,\n",
       " 'five': 348,\n",
       " 'morning': 349,\n",
       " 'looking': 350,\n",
       " 'second': 351,\n",
       " 'her,': 352,\n",
       " 'feet': 353,\n",
       " 'next': 354,\n",
       " 'close': 355,\n",
       " 'read': 356,\n",
       " 'vast': 357,\n",
       " 'man,': 358,\n",
       " 'me;': 359,\n",
       " 'sense': 360,\n",
       " 'them.': 361,\n",
       " 'eye': 362,\n",
       " 'given': 363,\n",
       " 'continued': 364,\n",
       " 'led': 365,\n",
       " 'merely': 366,\n",
       " 'is,': 367,\n",
       " 'hours': 368,\n",
       " 'scene': 369,\n",
       " 'immediately': 370,\n",
       " 'side': 371,\n",
       " 'get': 372,\n",
       " 'hideous': 373,\n",
       " 'call': 374,\n",
       " 'natural': 375,\n",
       " 'this,': 376,\n",
       " 'sight': 377,\n",
       " 'all,': 378,\n",
       " 'held': 379,\n",
       " 'night,': 380,\n",
       " 'account': 381,\n",
       " 'mere': 382,\n",
       " 'period': 383,\n",
       " 'reason': 384,\n",
       " 'filled': 385,\n",
       " 'moon': 386,\n",
       " 'four': 387,\n",
       " 'course': 388,\n",
       " 'remained': 389,\n",
       " 'hour': 390,\n",
       " 'stone': 391,\n",
       " 'themselves': 392,\n",
       " 'unknown': 393,\n",
       " 'speak': 394,\n",
       " '\"you': 395,\n",
       " 'doubt': 396,\n",
       " 'past': 397,\n",
       " 'true': 398,\n",
       " 'appearance': 399,\n",
       " 'sat': 400,\n",
       " 'best': 401,\n",
       " 'work': 402,\n",
       " 'change': 403,\n",
       " 'thoughts': 404,\n",
       " 'hear': 405,\n",
       " 'enough': 406,\n",
       " 'i,': 407,\n",
       " 'street': 408,\n",
       " 'behind': 409,\n",
       " 'eyes,': 410,\n",
       " 'too,': 411,\n",
       " 'save': 412,\n",
       " 'leave': 413,\n",
       " 'neither': 414,\n",
       " 'view': 415,\n",
       " 'dreams': 416,\n",
       " 'reached': 417,\n",
       " 'late': 418,\n",
       " 'attention': 419,\n",
       " 'wind': 420,\n",
       " 'life,': 421,\n",
       " 'greater': 422,\n",
       " 'gentle': 423,\n",
       " 'word': 424,\n",
       " 'us,': 425,\n",
       " 'possible': 426,\n",
       " 'done': 427,\n",
       " 'evil': 428,\n",
       " 'formed': 429,\n",
       " 'home': 430,\n",
       " 'feeling': 431,\n",
       " 'scarcely': 432,\n",
       " 'not,': 433,\n",
       " 'fellow': 434,\n",
       " 'wish': 435,\n",
       " 'beauty': 436,\n",
       " 'others': 437,\n",
       " 'friend,': 438,\n",
       " 'along': 439,\n",
       " 'living': 440,\n",
       " 'town': 441,\n",
       " 'somewhat': 442,\n",
       " 'across': 443,\n",
       " 'followed': 444,\n",
       " 'portion': 445,\n",
       " 'day,': 446,\n",
       " 'whether': 447,\n",
       " 'interest': 448,\n",
       " 'early': 449,\n",
       " 'feelings': 450,\n",
       " 'walls': 451,\n",
       " 'west': 452,\n",
       " 'really': 453,\n",
       " 'window': 454,\n",
       " 'dream': 455,\n",
       " 'latter': 456,\n",
       " 'family': 457,\n",
       " 'kept': 458,\n",
       " 'sweet': 459,\n",
       " 'various': 460,\n",
       " 'cold': 461,\n",
       " \"o'\": 462,\n",
       " 'short': 463,\n",
       " 'times': 464,\n",
       " 'low': 465,\n",
       " 'peculiar': 466,\n",
       " 'life.': 467,\n",
       " 'forth': 468,\n",
       " 'knowledge': 469,\n",
       " 'wonder': 470,\n",
       " 'trees': 471,\n",
       " 'hands': 472,\n",
       " 'hand,': 473,\n",
       " 'placed': 474,\n",
       " '\"and': 475,\n",
       " 'green': 476,\n",
       " 'twenty': 477,\n",
       " 'adrian': 478,\n",
       " 'perdita': 479,\n",
       " 'died': 480,\n",
       " 'finally': 481,\n",
       " 'especially': 482,\n",
       " 'dr.': 483,\n",
       " 'sleep': 484,\n",
       " 'discovered': 485,\n",
       " 'fact': 486,\n",
       " 'cast': 487,\n",
       " 'secret': 488,\n",
       " 'myself,': 489,\n",
       " 'cause': 490,\n",
       " 'distant': 491,\n",
       " 'memory': 492,\n",
       " 'self': 493,\n",
       " 'character': 494,\n",
       " 'evening': 495,\n",
       " 'god': 496,\n",
       " 'single': 497,\n",
       " 'necessary': 498,\n",
       " 'singular': 499,\n",
       " 'possessed': 500,\n",
       " 'sought': 501,\n",
       " 'course,': 502,\n",
       " 'returned': 503,\n",
       " 'use': 504,\n",
       " 'before,': 505,\n",
       " 'tried': 506,\n",
       " 'beheld': 507,\n",
       " 'country': 508,\n",
       " 'space': 509,\n",
       " 'heavy': 510,\n",
       " 'case': 511,\n",
       " 'ordinary': 512,\n",
       " 'altogether': 513,\n",
       " 'hundred': 514,\n",
       " 'met': 515,\n",
       " 'struck': 516,\n",
       " 'observed': 517,\n",
       " 'tears': 518,\n",
       " 'received': 519,\n",
       " 'ground': 520,\n",
       " 'here,': 521,\n",
       " 'mother': 522,\n",
       " 'sure': 523,\n",
       " 'does': 524,\n",
       " 'entirely': 525,\n",
       " 'floor': 526,\n",
       " 'ill': 527,\n",
       " 'herself': 528,\n",
       " 'six': 529,\n",
       " 'thrown': 530,\n",
       " 'shadow': 531,\n",
       " 'countenance': 532,\n",
       " 'spot': 533,\n",
       " 'replied': 534,\n",
       " 'loved': 535,\n",
       " 'happiness': 536,\n",
       " 'round': 537,\n",
       " 'fully': 538,\n",
       " 'blood': 539,\n",
       " 'impossible': 540,\n",
       " 'lovely': 541,\n",
       " 'regard': 542,\n",
       " 'got': 543,\n",
       " 'lived': 544,\n",
       " 'truth': 545,\n",
       " 'real': 546,\n",
       " 'together': 547,\n",
       " 'happy': 548,\n",
       " 'year': 549,\n",
       " 'land': 550,\n",
       " 'rest': 551,\n",
       " 'usual': 552,\n",
       " 'hardly': 553,\n",
       " 'line': 554,\n",
       " 'hold': 555,\n",
       " 'gone': 556,\n",
       " 'windows': 557,\n",
       " 'resolved': 558,\n",
       " 'bring': 559,\n",
       " 'opened': 560,\n",
       " 'question': 561,\n",
       " 'earth,': 562,\n",
       " 'keep': 563,\n",
       " 'caused': 564,\n",
       " 'care': 565,\n",
       " 'her.': 566,\n",
       " 'ye': 567,\n",
       " 'expression': 568,\n",
       " 'thy': 569,\n",
       " 'wide': 570,\n",
       " 'ten': 571,\n",
       " 'common': 572,\n",
       " 'difficulty': 573,\n",
       " 'house,': 574,\n",
       " 'apparently': 575,\n",
       " 'extreme': 576,\n",
       " 'beautiful': 577,\n",
       " 'former': 578,\n",
       " 'river': 579,\n",
       " 'sufficient': 580,\n",
       " 'circumstances': 581,\n",
       " 'slight': 582,\n",
       " 'lady': 583,\n",
       " 'perceived': 584,\n",
       " 'desire': 585,\n",
       " 'drew': 586,\n",
       " 'horrible': 587,\n",
       " 'threw': 588,\n",
       " 'entire': 589,\n",
       " 'seek': 590,\n",
       " 'covered': 591,\n",
       " 'youth': 592,\n",
       " 'up,': 593,\n",
       " 'age': 594,\n",
       " 'when,': 595,\n",
       " 'easily': 596,\n",
       " 'remember': 597,\n",
       " 'rose': 598,\n",
       " 'subject': 599,\n",
       " 'native': 600,\n",
       " 'coming': 601,\n",
       " 'length,': 602,\n",
       " 'him;': 603,\n",
       " 'live': 604,\n",
       " 'bore': 605,\n",
       " 'steps': 606,\n",
       " 'making': 607,\n",
       " 'death.': 608,\n",
       " 'fresh': 609,\n",
       " 'sounds': 610,\n",
       " 'therefore,': 611,\n",
       " 'dared': 612,\n",
       " 'heart,': 613,\n",
       " 'months': 614,\n",
       " 'evidently': 615,\n",
       " 'child': 616,\n",
       " 'before.': 617,\n",
       " 'effect': 618,\n",
       " 'distance': 619,\n",
       " 'clear': 620,\n",
       " 'cut': 621,\n",
       " 'public': 622,\n",
       " 'taking': 623,\n",
       " 'excited': 624,\n",
       " 'turn': 625,\n",
       " 'force': 626,\n",
       " 'gilman': 627,\n",
       " 'he,': 628,\n",
       " 'anything': 629,\n",
       " 'attempt': 630,\n",
       " 'figure': 631,\n",
       " 'able': 632,\n",
       " 'narrow': 633,\n",
       " 'seized': 634,\n",
       " 'presence': 635,\n",
       " 'degree': 636,\n",
       " 'rendered': 637,\n",
       " 'blue': 638,\n",
       " 'order': 639,\n",
       " 'madame': 640,\n",
       " 'joy': 641,\n",
       " 'mean': 642,\n",
       " \"don't\": 643,\n",
       " 'hung': 644,\n",
       " 'apparent': 645,\n",
       " 'miserable': 646,\n",
       " 'letter': 647,\n",
       " 'except': 648,\n",
       " 'sent': 649,\n",
       " 'thou': 650,\n",
       " 'surface': 651,\n",
       " 'one,': 652,\n",
       " 'imagination': 653,\n",
       " 'heaven': 654,\n",
       " 'north': 655,\n",
       " 'grief': 656,\n",
       " '\"it': 657,\n",
       " 'different': 658,\n",
       " 'wall': 659,\n",
       " 'terror': 660,\n",
       " 'houses': 661,\n",
       " 'red': 662,\n",
       " 'greatest': 663,\n",
       " 'later': 664,\n",
       " 'words,': 665,\n",
       " 'misery': 666,\n",
       " 'arose': 667,\n",
       " 'fact,': 668,\n",
       " 'broken': 669,\n",
       " 'all.': 670,\n",
       " 'proceeded': 671,\n",
       " 'utterly': 672,\n",
       " 'going': 673,\n",
       " 'similar': 674,\n",
       " 'queer': 675,\n",
       " 'silent': 676,\n",
       " 'main': 677,\n",
       " 'visible': 678,\n",
       " 'faint': 679,\n",
       " 'fancy': 680,\n",
       " 'arrived': 681,\n",
       " 'leaving': 682,\n",
       " 'simple': 683,\n",
       " 'st.': 684,\n",
       " 'prepared': 685,\n",
       " 'suffered': 686,\n",
       " 'idris': 687,\n",
       " 'despair': 688,\n",
       " 'number': 689,\n",
       " 'lord': 690,\n",
       " 'odd': 691,\n",
       " 'fallen': 692,\n",
       " 'help': 693,\n",
       " 'pass': 694,\n",
       " 'visit': 695,\n",
       " '\"but': 696,\n",
       " 'room,': 697,\n",
       " 'existence': 698,\n",
       " 'appear': 699,\n",
       " 'beloved': 700,\n",
       " 'arm': 701,\n",
       " 'sky': 702,\n",
       " 'silence': 703,\n",
       " 'shewed': 704,\n",
       " 'pleasure': 705,\n",
       " 'chamber': 706,\n",
       " 'art': 707,\n",
       " 'm.': 708,\n",
       " 'again,': 709,\n",
       " 'were,': 710,\n",
       " 'deserted': 711,\n",
       " 'head,': 712,\n",
       " 'darkness': 713,\n",
       " 'fire': 714,\n",
       " 'man.': 715,\n",
       " 'alas': 716,\n",
       " 'closed': 717,\n",
       " 'actually': 718,\n",
       " 'strong': 719,\n",
       " 'spent': 720,\n",
       " 'vague': 721,\n",
       " 'sole': 722,\n",
       " 'considered': 723,\n",
       " 'woman': 724,\n",
       " 'third': 725,\n",
       " 'delight': 726,\n",
       " 'asked': 727,\n",
       " 'walked': 728,\n",
       " 'remain': 729,\n",
       " 'mountain': 730,\n",
       " 'escape': 731,\n",
       " 'influence': 732,\n",
       " 'determined': 733,\n",
       " 'uncle': 734,\n",
       " 'door,': 735,\n",
       " 'carried': 736,\n",
       " 'wished': 737,\n",
       " 'arms': 738,\n",
       " 'exceedingly': 739,\n",
       " 'says': 740,\n",
       " 'immediate': 741,\n",
       " 'hill': 742,\n",
       " 'o': 743,\n",
       " 'whilst': 744,\n",
       " 'thought,': 745,\n",
       " 'pale': 746,\n",
       " 'golden': 747,\n",
       " 'himself,': 748,\n",
       " 'place,': 749,\n",
       " 'sufficiently': 750,\n",
       " 'affection': 751,\n",
       " 'understand': 752,\n",
       " 'purpose': 753,\n",
       " 'step': 754,\n",
       " 'need': 755,\n",
       " 'lips': 756,\n",
       " 'forced': 757,\n",
       " 'stars': 758,\n",
       " 'occupied': 759,\n",
       " 'strength': 760,\n",
       " 'huge': 761,\n",
       " 'friends': 762,\n",
       " 'objects': 763,\n",
       " 'quickly': 764,\n",
       " 'reach': 765,\n",
       " 'free': 766,\n",
       " 'passage': 767,\n",
       " 'pain': 768,\n",
       " 'mad': 769,\n",
       " 'below': 770,\n",
       " 'following': 771,\n",
       " 'supposed': 772,\n",
       " 'father,': 773,\n",
       " 'corpse': 774,\n",
       " 'england': 775,\n",
       " 'therefore': 776,\n",
       " 'changed': 777,\n",
       " 'who,': 778,\n",
       " 'motion': 779,\n",
       " 'moved': 780,\n",
       " 'party': 781,\n",
       " 'beside': 782,\n",
       " 'business': 783,\n",
       " 'there,': 784,\n",
       " 'nature,': 785,\n",
       " 'perceive': 786,\n",
       " 'us.': 787,\n",
       " 'perfect': 788,\n",
       " 'used': 789,\n",
       " 'spread': 790,\n",
       " 'love,': 791,\n",
       " 'concerning': 792,\n",
       " 'death,': 793,\n",
       " 'learned': 794,\n",
       " 'plague': 795,\n",
       " 'whatever': 796,\n",
       " 'frightful': 797,\n",
       " 'meet': 798,\n",
       " 'world,': 799,\n",
       " 'born': 800,\n",
       " 'probably': 801,\n",
       " 'time.': 802,\n",
       " 'tale': 803,\n",
       " 'unusual': 804,\n",
       " 'amidst': 805,\n",
       " 'dare': 806,\n",
       " 'it;': 807,\n",
       " 'mighty': 808,\n",
       " 'night.': 809,\n",
       " 'watched': 810,\n",
       " 'curious': 811,\n",
       " 'streets': 812,\n",
       " 'direction': 813,\n",
       " 'intense': 814,\n",
       " 'events': 815,\n",
       " 'men,': 816,\n",
       " 'top': 817,\n",
       " 'forgotten': 818,\n",
       " 'bottom': 819,\n",
       " 'fall': 820,\n",
       " 'sort': 821,\n",
       " 'children': 822,\n",
       " 'voice,': 823,\n",
       " 'die': 824,\n",
       " 'book': 825,\n",
       " 'mind,': 826,\n",
       " 'again.': 827,\n",
       " 'certainly': 828,\n",
       " 'talked': 829,\n",
       " 'farther': 830,\n",
       " 'extent': 831,\n",
       " 'grave': 832,\n",
       " 'miles': 833,\n",
       " 'act': 834,\n",
       " 'bed': 835,\n",
       " 'gentleman': 836,\n",
       " 'table': 837,\n",
       " 'unable': 838,\n",
       " 'had,': 839,\n",
       " 'bear': 840,\n",
       " 'hair': 841,\n",
       " 'behold': 842,\n",
       " 'fine': 843,\n",
       " 'however': 844,\n",
       " 'giving': 845,\n",
       " 'box': 846,\n",
       " 'presented': 847,\n",
       " 'seem': 848,\n",
       " 'mental': 849,\n",
       " 'bent': 850,\n",
       " 'occurred': 851,\n",
       " 'believed': 852,\n",
       " 'condition': 853,\n",
       " 'trace': 854,\n",
       " 'physical': 855,\n",
       " 'material': 856,\n",
       " 'utter': 857,\n",
       " 'balloon': 858,\n",
       " 'hopes': 859,\n",
       " 'sir': 860,\n",
       " 'brief': 861,\n",
       " 'earth.': 862,\n",
       " 'things,': 863,\n",
       " 'future': 864,\n",
       " 'original': 865,\n",
       " 'else': 866,\n",
       " 'ran': 867,\n",
       " 'evidence': 868,\n",
       " 'suppose': 869,\n",
       " 'sudden': 870,\n",
       " 'seven': 871,\n",
       " 'regarded': 872,\n",
       " 'endeavoured': 873,\n",
       " 'rise': 874,\n",
       " 'visited': 875,\n",
       " 'proved': 876,\n",
       " 'bitter': 877,\n",
       " 'personal': 878,\n",
       " 'circumstance': 879,\n",
       " 'listened': 880,\n",
       " 'marked': 881,\n",
       " 'proper': 882,\n",
       " 'burst': 883,\n",
       " 'slowly': 884,\n",
       " 'feared': 885,\n",
       " 'creature': 886,\n",
       " 'calm': 887,\n",
       " 'heart.': 888,\n",
       " 'village': 889,\n",
       " 'melancholy': 890,\n",
       " 'aware': 891,\n",
       " 'carefully': 892,\n",
       " 'succeeded': 893,\n",
       " 'progress': 894,\n",
       " 'oh': 895,\n",
       " 'outside': 896,\n",
       " 'design': 897,\n",
       " 'grey': 898,\n",
       " 'wife': 899,\n",
       " 'forms': 900,\n",
       " 'fearful': 901,\n",
       " 'books': 902,\n",
       " 'gigantic': 903,\n",
       " 'sea,': 904,\n",
       " 'hard': 905,\n",
       " 'fair': 906,\n",
       " 'want': 907,\n",
       " 'manner,': 908,\n",
       " 'remarkable': 909,\n",
       " 'remote': 910,\n",
       " 'gods': 911,\n",
       " 'foot': 912,\n",
       " 'itself,': 913,\n",
       " 'fancied': 914,\n",
       " 'burning': 915,\n",
       " 'vain': 916,\n",
       " 'assured': 917,\n",
       " 'watch': 918,\n",
       " \"man's\": 919,\n",
       " 'talk': 920,\n",
       " 'wholly': 921,\n",
       " 'fifty': 922,\n",
       " 'upper': 923,\n",
       " 'aid': 924,\n",
       " 'minutes': 925,\n",
       " 'gold': 926,\n",
       " 'spring': 927,\n",
       " 'years,': 928,\n",
       " 'once,': 929,\n",
       " 'instant': 930,\n",
       " 'atmosphere': 931,\n",
       " 'raymond,': 932,\n",
       " 'result': 933,\n",
       " 'hidden': 934,\n",
       " 'soft': 935,\n",
       " '\"my': 936,\n",
       " 'way,': 937,\n",
       " 'despite': 938,\n",
       " 'street,': 939,\n",
       " 'seems': 940,\n",
       " 'boat': 941,\n",
       " 'so,': 942,\n",
       " 'search': 943,\n",
       " 'perhaps,': 944,\n",
       " 'lead': 945,\n",
       " 'rain': 946,\n",
       " 'big': 947,\n",
       " 'answer': 948,\n",
       " 'ship': 949,\n",
       " 'noticed': 950,\n",
       " 'ask': 951,\n",
       " 'animal': 952,\n",
       " 'remembered': 953,\n",
       " 'company': 954,\n",
       " 'gradually': 955,\n",
       " 'on,': 956,\n",
       " 'enter': 957,\n",
       " 'madness': 958,\n",
       " 'fixed': 959,\n",
       " 'story': 960,\n",
       " 'drawn': 961,\n",
       " 'leading': 962,\n",
       " 'paper': 963,\n",
       " 'shut': 964,\n",
       " 'eight': 965,\n",
       " 'waters': 966,\n",
       " 'noble': 967,\n",
       " 'expected': 968,\n",
       " 'eyes.': 969,\n",
       " 'dark,': 970,\n",
       " 'discovery': 971,\n",
       " 'follow': 972,\n",
       " 'machine': 973,\n",
       " 'try': 974,\n",
       " 'grown': 975,\n",
       " 'ought': 976,\n",
       " 'lower': 977,\n",
       " 'mind.': 978,\n",
       " 'corner': 979,\n",
       " 'absence': 980,\n",
       " 'south': 981,\n",
       " 'ghastly': 982,\n",
       " 'oh,': 983,\n",
       " 'bodies': 984,\n",
       " 'marble': 985,\n",
       " 'particular': 986,\n",
       " 'long,': 987,\n",
       " 'features': 988,\n",
       " 'boy': 989,\n",
       " 'sorrow': 990,\n",
       " 'alive': 991,\n",
       " '\"': 992,\n",
       " 're': 993,\n",
       " 'out,': 994,\n",
       " 'mentioned': 995,\n",
       " 'light,': 996,\n",
       " 'clouds': 997,\n",
       " 'greatly': 998,\n",
       " 'le': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('vocab length:',len(vocab_to_int))\n",
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里测试一下函数的结果对不对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.\n",
      "text_ints: [  111   541    23   927    15    32   197    21  1708  8426    24     0\n",
      "  2966  3391 12787   790  8427 12788    22   548  9515     2 19631  6862\n",
      "    31   197    15     6   578   928   184 10910     2  9516]\n"
     ]
    }
   ],
   "source": [
    "test_a = data.iloc[3,1]\n",
    "test_b = text_to_ints(test_a)\n",
    "print('text:',test_a)\n",
    "print('text_ints:',test_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以train_X就这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X samples: [ array([   25, 10909,   138,  1282,    29,    35,   268,     1,  8424,\n",
      "           0,  5809,     1,     9, 27735,    15,     5,    76,   166,\n",
      "          43, 27736,     2,   345,     3,     0,   292,  2286,     5,\n",
      "         302,   994,   120,   124,   891,     1,     0, 27737,    38,\n",
      "        1348,  5016,    89,     0,  2838])\n",
      " array([  12,   86,  131,  851,    3,   29,    8,    0, 6308,   76,   26,\n",
      "          4,  382, 8425])\n",
      " array([    6,    14,   152,   263,     7,     4,   926,  7538,  2073,\n",
      "          21,   283,    15,    13, 19630,   116,     0,  2624,  5017,\n",
      "          31,   337,     1,  1283,  2965,    13,   190,  7538,  9514,\n",
      "          11,    34,   247,     1,     0,   663,   426,   493, 15414])\n",
      " array([  111,   541,    23,   927,    15,    32,   197,    21,  1708,\n",
      "        8426,    24,     0,  2966,  3391, 12787,   790,  8427, 12788,\n",
      "          22,   548,  9515,     2, 19631,  6862,    31,   197,    15,\n",
      "           6,   578,   928,   184, 10910,     2,  9516])\n",
      " array([ 1041,   169,  6863,    20,    67,  3392,     0,  6864,  1530,\n",
      "          14, 27738,    18,     4,  8428,   243,  1655, 19632,    84,\n",
      "          14,   532,    15,    13,  6865,  1493,    19,    14, 15415])]\n",
      "max length: 861\n"
     ]
    }
   ],
   "source": [
    "train_X = []\n",
    "text_length = []\n",
    "for each in data['text']:\n",
    "    train_X.append(text_to_ints(each))\n",
    "    text_length.append(len(text_to_ints(each)))\n",
    "text_length = np.array(text_length)\n",
    "train_X = np.array(train_X)\n",
    "print('train_X samples:',train_X[:5])\n",
    "print('max length:',text_length.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y samples:     0  1  2\n",
      "0   1  0  0\n",
      "1   0  1  0\n",
      "2   1  0  0\n",
      "3   0  0  1\n",
      "4   0  1  0\n",
      "5   0  0  1\n",
      "6   1  0  0\n",
      "7   1  0  0\n",
      "8   1  0  0\n",
      "9   0  0  1\n",
      "10  0  0  1\n",
      "11  1  0  0\n",
      "12  0  1  0\n",
      "13  0  1  0\n",
      "14  1  0  0\n",
      "15  0  0  1\n",
      "16  1  0  0\n",
      "17  0  0  1\n",
      "18  1  0  0\n",
      "19  0  1  0\n",
      "train_y shape: (19579, 3)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for each in data['author']:#列表表达式貌似不能写三分支，只好使用很笨的for循环\n",
    "    if each=='EAP':\n",
    "        labels.append(0)\n",
    "    elif each=='HPL':\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(2)\n",
    "#labels = np.array([0 if each == 'EAP' 1 if each=='HPL' else 2 for each in labels])\n",
    "int_to_author={0:'EAP',1:'HPL',2:'MWS'}\n",
    "train_y = np.array(labels)\n",
    "train_y = pd.get_dummies(train_y)\n",
    "print('train_y samples:',train_y[:20])\n",
    "print('train_y shape:',train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的数据可以看出，text中最长有861个字，为了提高效率，我们还是选取200个字的长度作为输入，多退少补"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...,    89     0  2838]\n",
      " [    0     0     0 ...,     4   382  8425]\n",
      " [    0     0     0 ...,   426   493 15414]\n",
      " ..., \n",
      " [    0     0     0 ...,    21     9  3552]\n",
      " [    0     0     0 ...,    46   102  2441]\n",
      " [    0     0     0 ..., 12792    16   140]]\n",
      "(19579, 500)\n"
     ]
    }
   ],
   "source": [
    "def trunc_seq(seq,seq_len=500):\n",
    "    features = np.zeros((len(seq), seq_len), dtype=int)\n",
    "    for i, row in enumerate(seq):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_len]\n",
    "    return features\n",
    "train_X = trunc_seq(train_X)\n",
    "print(train_X[:10])\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 325\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = len(vocab_to_int)\n",
    "\n",
    "# Create the graph object\n",
    "graph = tf.Graph()\n",
    "# Add nodes to the graph\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    labels_ = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53248"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要先做word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300 \n",
    "\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # Add dropout to the cell\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed,\n",
    "                                             initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 3, activation_fn=tf.nn.softmax)\n",
    "    #cost = tf.losses.mean_squared_error(labels_, predictions)\n",
    "    cost = tf.losses.softmax_cross_entropy(labels_,predictions)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 预测test集\n",
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels_)\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batching\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/15 Iteration: 5 Train loss: 1.097\n",
      "Epoch: 0/15 Iteration: 10 Train loss: 1.099\n",
      "Epoch: 0/15 Iteration: 15 Train loss: 1.097\n"
     ]
    }
   ],
   "source": [
    "epochs = 15\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_X, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y,\n",
    "                    keep_prob: 0.6,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {:.3f}\".format(loss))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/sentiment.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "规整数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_text_length = []\n",
    "for each in test_data['text']:\n",
    "    test_X.append(text_to_ints(each))\n",
    "    test_text_length.append(len(text_to_ints(each)))\n",
    "test_text_length = np.array(text_length)\n",
    "print('test_X samples:',test_X[:5])\n",
    "print('max length:',text_length.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "zeros = np.array([0,0,0])\n",
    "for i in range(9000-len(test_X)):\n",
    "    test_X.append(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = np.array(test_X)\n",
    "test_X = trunc_seq(test_X)\n",
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_get_batches(x, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x = x[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_X)\n",
    "n_batches = len(test_X)//batch_size\n",
    "state_size = len(test_X)-n_batches*batch_size\n",
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    test_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "    for ii, x in enumerate(test_get_batches(test_X, batch_size), 1):\n",
    "        feed = {inputs_: x,\n",
    "                keep_prob: 1,\n",
    "                initial_state: test_state}\n",
    "        predict = sess.run(predictions, feed_dict=feed)\n",
    "        predicts.append(predict)\n",
    "    #n_batches = len(test_X//batch_size)\n",
    "    #state_size = len(test_X)-n_batches*batch_size\n",
    "    #test_state_2 = sess.run(cell.zero_state(392,tf.float32))\n",
    "    #predicts.append(sess.run(predictions,feed_dict={inputs_:test_X[n_batches*batch_size:],\n",
    "                                                   #keep_prob:1,\n",
    "                                                   #initial_state:test_state_2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('batch shape:',predicts[1].shape)\n",
    "print('predict length:',len(predicts))\n",
    "print('predict sample:\\n',predicts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到predicts中每一个item都是一个batch的array，这里还需要将结果处理一下，根据格式要求，要求结果为每个作者的可能性，所以不需要预测到具体某个作者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for each in predicts:\n",
    "    for i in range(each.shape[0]):\n",
    "        prob = each[i,:]\n",
    "        result.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('result length:',len(result))\n",
    "print('result sample:',result[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill_result = np.zeros((len(result),3))\n",
    "#for i in range(len(result)):\n",
    "    #idx = result[i].argmax()\n",
    "    #fill_result[i,idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "按照要求的格式将结果整理成DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = np.array(result)\n",
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result_array,dtype=float)\n",
    "result_df.columns=['EAP','HPL','MWS']\n",
    "result_df = result_df[:len(test_data['id'])]\n",
    "result_df = result_df.applymap(lambda x: '%.15f' % x)\n",
    "result_df.insert(0,'id',test_data['id'])\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面只要将结果写入到csv文件中就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_df.to_csv('submision.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlnd]",
   "language": "python",
   "name": "conda-env-dlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
